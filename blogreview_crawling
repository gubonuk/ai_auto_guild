# 필요한 라이브러리 설치 (터미널에서 실행)
# pip install selenium webdriver-manager beautifulsoup4

from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time
import os
import re


def fetch_blog_post_html(url: str) -> str:
    """
    Selenium을 사용하여 네이버 블로그 글의 HTML 소스를 가져오는 함수입니다.
    매개변수:
      url (str): 가져올 네이버 블로그 글의 URL
    반환:
      str: 블로그 글의 HTML 소스
    """
    options = webdriver.ChromeOptions()
    options.add_argument("--headless")  # 브라우저 창 없이 실행
    options.add_argument("--disable-gpu")  # GPU 사용 안함
    options.add_argument("--no-sandbox")  # 샌드박스 비활성화
    options.add_argument("--disable-dev-shm-usage")  # 공유 메모리 사용 비활성화
    options.add_argument("--disable-extensions")  # 확장 프로그램 비활성화

    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)

    try:
        driver.get(url)
        # iframe 요소가 로드될 때까지 최대 10초 대기
        iframe = WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.ID, "mainFrame"))
        )
        driver.switch_to.frame(iframe)

        # iframe 내 콘텐츠(body)가 로드될 때까지 대기
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.TAG_NAME, "body"))
        )

        html = driver.page_source
        return html
    except Exception as e:
        print(f"URL {url} 처리 중 오류 발생: {e}")
        return ""
    finally:
        driver.quit()


def extract_text_from_html(html: str) -> str:
    """
    HTML 소스에서 블로그 글 내용만 정밀하게 추출하는 함수입니다.
    매개변수:
      html (str): 블로그 글의 HTML 소스
    반환:
      str: 추출된 블로그 글 텍스트 내용 (깔끔하게 정리됨)
    """
    if not html:
        return ""

    soup = BeautifulSoup(html, 'html.parser')

    # 타이틀 추출
    title = ""
    title_element = soup.select_one('div.se-title-text, h3.se_title, span.se-fs-')
    if title_element:
        title = title_element.get_text(strip=True)

    extracted_text = []

    # 1. 새 에디터 버전 (se-main-container)
    if soup.select_one('div.se-main-container'):
        # 본문 텍스트 블록 추출
        for text_block in soup.select('div.se-text'):
            paragraph = text_block.get_text(strip=True)
            if paragraph:
                extracted_text.append(paragraph)

        # 인용구 추출
        for quote in soup.select('div.se-quote'):
            quote_text = quote.get_text(strip=True)
            if quote_text:
                extracted_text.append(f"인용: {quote_text}")

        # 코드 블록 추출
        for code in soup.select('div.se-code'):
            code_text = code.get_text(strip=True)
            if code_text:
                extracted_text.append(f"코드: {code_text}")

        # 표 추출 (간단하게 텍스트만)
        for table in soup.select('table.se-table'):
            for row in table.select('tr'):
                row_texts = [cell.get_text(strip=True) for cell in row.select('th, td')]
                if row_texts:
                    extracted_text.append(' | '.join(row_texts))

    # 2. 구 에디터 버전
    elif soup.select_one('div.__content_area__, div.post-view'):
        content_area = soup.select_one('div.__content_area__, div.post-view, div#postViewArea')
        if content_area:
            # 스크립트, 스타일 태그 제거
            for tag in content_area.select('script, style'):
                tag.decompose()

            # 단락 추출
            for p in content_area.select('p'):
                p_text = p.get_text(strip=True)
                if p_text and len(p_text) > 1:  # 의미있는 텍스트만 추출
                    extracted_text.append(p_text)

            # 단락 태그가 없는 경우, 직접 텍스트 노드 추출
            if not extracted_text:
                text_chunks = []
                for element in content_area.contents:
                    if element.name not in ['script', 'style'] and element.string and element.string.strip():
                        text_chunks.append(element.string.strip())

                if text_chunks:
                    extracted_text = text_chunks
                else:
                    # 최후의 수단: 전체 텍스트를 가져와서 정리
                    full_text = content_area.get_text(separator='\n')
                    extracted_text = [line.strip() for line in full_text.split('\n') if line.strip()]

    # 최소한의 텍스트도 찾지 못한 경우
    if not extracted_text:
        # 페이지에서 의미있는 텍스트 블록 찾기
        for tag in soup.select('p, div, span'):
            text = tag.get_text(strip=True)
            if text and len(text) > 50:  # 일정 길이 이상인 텍스트 블록만 고려
                extracted_text.append(text)

    # 중복 제거 및 정리
    cleaned_text = []
    seen = set()
    for text in extracted_text:
        text = text.strip()
        # 짧은 단편 텍스트나 중복 제거
        if text and len(text) > 5 and text not in seen:
            cleaned_text.append(text)
            seen.add(text)

    # 최종 결과 조합
    if title:
        final_text = f"제목: {title}\n\n" + "\n\n".join(cleaned_text)
    else:
        final_text = "\n\n".join(cleaned_text)

    # 불필요한 공백 처리
    final_text = re.sub(r'\n{3,}', '\n\n', final_text)

    return final_text


def get_blog_metadata(html: str) -> dict:
    """
    HTML에서 블로그 글의 메타데이터를 추출합니다.
    """
    if not html:
        return {}

    soup = BeautifulSoup(html, 'html.parser')
    metadata = {}

    # 제목 추출
    title_selectors = [
        'div.se-title-text span.se-fs-',
        'h3.se_title span',
        'h3.se_textarea',
        'div.tit_area h3',
        'title'
    ]

    for selector in title_selectors:
        title_element = soup.select_one(selector)
        if title_element:
            metadata['title'] = title_element.get_text(strip=True)
            break

    # 작성일 추출
    date_selectors = [
        'span.se_publishDate',
        'span.date',
        'p.blog_date',
        'div.date'
    ]

    for selector in date_selectors:
        date_element = soup.select_one(selector)
        if date_element:
            metadata['date'] = date_element.get_text(strip=True)
            break

    # 카테고리 추출
    category_selectors = [
        'div.blog_category',
        'a.category',
        'span.cate'
    ]

    for selector in category_selectors:
        category_element = soup.select_one(selector)
        if category_element:
            metadata['category'] = category_element.get_text(strip=True)
            break

    return metadata


def save_text_to_file(text: str, filename: str):
    """
    추출된 텍스트를 파일로 저장하는 함수입니다.
    매개변수:
      text (str): 저장할 텍스트 내용
      filename (str): 저장할 파일 이름
    """
    with open(filename, 'w', encoding='utf-8') as f:
        f.write(text)
    print(f"텍스트가 {filename}에 저장되었습니다. (크기: {len(text)} 바이트)")


def process_url_list(url_list: list, output_dir: str = "extracted_blogs"):
    """
    URL 리스트를 처리하여 각 블로그 글의 내용을 텍스트로 추출하고 저장하는 함수입니다.
    매개변수:
      url_list (list): 처리할 네이버 블로그 URL 리스트
      output_dir (str): 추출한 텍스트를 저장할 디렉토리
    """
    # 출력 디렉토리 생성
    os.makedirs(output_dir, exist_ok=True)

    total_urls = len(url_list)
    successful = 0
    failed = 0

    for i, url in enumerate(url_list):
        print(f"[{i + 1}/{total_urls}] URL 처리 중: {url}")

        try:
            # URL에서 고유 식별자 추출 (URL의 마지막 부분)
            blog_id = url.split('/')[-2]
            post_id = url.split('/')[-1]
            filename = f"{output_dir}/{blog_id}_{post_id}.txt"

            # 이미 처리한 URL인지 확인
            if os.path.exists(filename):
                print(f"이미 처리된 URL입니다. 파일: {filename}")
                successful += 1
                continue

            # HTML 가져오기
            html = fetch_blog_post_html(url)

            if not html:
                print(f"HTML을 가져오지 못했습니다: {url}")
                failed += 1
                continue

            # 메타데이터 추출
            metadata = get_blog_metadata(html)

            # 텍스트 추출
            content = extract_text_from_html(html)

            # 메타데이터와 콘텐츠 결합
            full_text = ""
            if 'title' in metadata:
                full_text += f"제목: {metadata['title']}\n"
            if 'date' in metadata:
                full_text += f"작성일: {metadata['date']}\n"
            if 'category' in metadata:
                full_text += f"카테고리: {metadata['category']}\n"

            if full_text:
                full_text += "\n" + "-" * 50 + "\n\n"

            full_text += content

            # 파일로 저장
            save_text_to_file(full_text, filename)
            successful += 1

            # 네이버 서버에 부담을 주지 않기 위한 딜레이
            time.sleep(2)

        except Exception as e:
            print(f"URL 처리 중 오류 발생: {url} - {str(e)}")
            failed += 1

    print(f"\n처리 완료:")
    print(f"  - 총 URL: {total_urls}")
    print(f"  - 성공: {successful}")
    print(f"  - 실패: {failed}")
    print(f"결과는 '{output_dir}' 디렉토리에 저장되었습니다.")


def load_urls_from_file(file_path: str) -> list:
    """
    텍스트 파일에서 URL 리스트를 로드합니다.
    매개변수:
      file_path (str): URL 리스트가 있는 텍스트 파일 경로
    반환:
      list: URL 리스트
    """
    with open(file_path, 'r', encoding='utf-8') as f:
        return [line.strip() for line in f if line.strip() and not line.startswith('#')]


if __name__ == "__main__":
    # URL 리스트 직접 지정
    url_list = [
        "https://blog.naver.com/hihihim1/223040368960",
        # 여기에 더 많은 URL 추가
    ]

    # 또는 파일에서 URL 리스트 로드
    # url_list = load_urls_from_file("naver_blog_urls.txt")

    # URL 리스트 처리
    process_url_list(url_list)
